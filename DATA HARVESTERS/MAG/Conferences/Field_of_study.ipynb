{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Field_of_study.ipynb","provenance":[],"authorship_tag":"ABX9TyOolRj6JkQbrDajS4XxRrGx"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"O08A5fh_ZHds","colab_type":"code","colab":{},"cellView":"form"},"source":["# @title ##### Get Conference Data\n","\n","\"\"\"\n","MAG - Microsoft Academic Graph\n","==============================\n","MAG provides data of all research papers.\n","\"\"\" \n","import json\n","import requests\n","from pprint import pprint\n","import csv\n","import pandas as pd\n","from datetime import date\n","import ast\n","\n","class MAG:\n","  \"\"\"\n","  Arguments:\n","  ---------\n","    - conference : Name of the conference we need to fetch data for\n","    - data_fetch : What kind of Data to fetch from MAG\n","    - start_date : start date of data\n","    - end_date : end date of data\n","    - retry : Number of attemts to connect to MAG\n","  \"\"\" \n","  def __init__(self, **kwargs):\n","    df = pd.read_excel(\"/content/drive/My Drive/AI Index Visualization Project/DATA HARVESTORS/MAG/Conferences/Conferences.xlsx\")\n","    self.e_d = date.today().strftime(\"%Y-%m-%d\")\n","    self.s_d = kwargs.get('start_date').split('-')\n","    self.e_d = kwargs.get('end_date').split('-')\n","    self.s_d = int(self.s_d[0])\n","    self.e_d = int(self.e_d[0])\n","    self.start_date = kwargs.get('start_date')\n","    self.end_date = kwargs.get('end_date')\n","    self.conference = kwargs.get('conference')\n","    self.data_fetch = kwargs.get('data_fetch') \n","    self.retry = kwargs.get('retry')\n","    self.conference_id = int(df[df['Conference Name']==self.conference]['Conference ID'])\n","    print(self.conference_id)\n","  def getData(self):\n","    \"\"\"\n","    Data from MAG.\n","\n","    RETURNS\n","    -------\n","    - Return dataframe containing MAG data\n","    \"\"\"\n","    from time import sleep\n","    retry = 0\n","    while(True):\n","        try:\n","          if(retry == self.retry):\n","            print(\"Failed to connect to MAG \\\n","                      please recheck the connection\")\n","            break\n","          endpoint = 'https://api.labs.cognitive.microsoft.com/'\n","          api_version = 'academic/v1.0/evaluate?'\n","          headers = {'Ocp-Apim-Subscription-Key': 'f4191fe719cf4659813a86e6830b2546',}\n","          searchstring = \"FN={}\".format(self.conference_id)\n","          url = endpoint + api_version + \"expr=\" + searchstring + \"&attributes=\" + \"&count=100000\" + \"&orderby=Y:asc\"\n","          response  = requests.get(url, headers=headers)\n","          query = response.json()\n","          self.query = query\n","          self.json_to_csv()\n","          self.data = self.basicPreprocess()\n","          if self.data_fetch == \"Author\" or self.data_fetch == \"Fields of Study\":\n","            self.data = self.grid()\n","          # print (self.query)\n","          return self.data\n","        except Exception as e:\n","                retry = retry + 1\n","                print(\"MAG Exception: \", e)\n","                print(\"Reattempting the server: \", retry)\n","                sleep(60)  \n","\n","  def json_to_csv(self):\n","    json_object = json.dumps(self.query, indent = 6) \n","    with open(\"mag.json\", \"w\") as outfile: \n","      outfile.write(json_object)     \n","    with open('mag.json') as json_file: \n","      data = json.load(json_file) \n","\n","    data_entities = data['entities'] \n","\n","    # now we will open a file for writing \n","    data_file = open('data_file.csv', 'w') \n","\n","    # create the csv writer object \n","    csv_writer = csv.writer(data_file) \n","\n","    # Counter variable used for writing \n","    # headers to the CSV file \n","    count = 0\n","\n","    for ent in data_entities: \n","      if count == 0: \n","\n","        # Writing headers of CSV file \n","        header = ent.keys() \n","        csv_writer.writerow(header) \n","        count += 1\n","\n","      # Writing data of CSV file \n","      csv_writer.writerow(ent.values()) \n","\n","    data_file.close()\n","\n","  def basicPreprocess(self):\n","    \"\"\"\n","    Performs basic preprocessing steps.\n","    \"\"\"\n","    da = pd.read_csv('data_file.csv')\n","    if self.data_fetch == \"Fields of Study\":\n","      da['F'] = da['F'].apply(ast.literal_eval)\n","      df1 = pd.concat({k:pd.DataFrame(v) for k, v in da['F'].items()})\n","      da = da.join(df1.reset_index(level=1, drop=True)).reset_index(drop=True)\n","      da = da.drop(['logprob','prob','F'],axis = 1)\n","      da = da.rename(columns={'FN':'Field_of_study','VSN':'Conference','Y':'Date'})\n","      da['AA'] = da['AA'].apply(ast.literal_eval)\n","      df1 = pd.concat({k:pd.DataFrame(v) for k, v in da['AA'].items()})\n","      da = da.join(df1.reset_index(level=1, drop=True)).reset_index(drop=True)\n","      da = da.drop(['AA'],axis = 1)\n","      da = da.rename(columns={'DAfN':'Affiliation'})\n","      da = da.dropna()\n","      da = da.groupby(['Date','Field_of_study','Affiliation']).agg('count')\n","      da = da.rename(columns={'Conference':'Publication_Count'})\n","      conf = [f'{self.conference}'] * len(da)\n","      da['Conference'] = conf\n","\n","    elif self.data_fetch == \"Author\":\n","      da['AA'] = da['AA'].apply(ast.literal_eval)\n","      df1 = pd.concat({k:pd.DataFrame(v) for k, v in da['AA'].items()})\n","      da = da.join(df1.reset_index(level=1, drop=True)).reset_index(drop=True)\n","      da = da.drop(['logprob','prob','AA'],axis = 1)\n","      da = da.rename(columns={'DAfN':'Affiliation','DAuN':'Author','VSN':'Conference','Y':'Date'})\n","\n","    elif self.data_fetch == \"Publication Count\":\n","      da = da.drop(['logprob','prob','PCS'],axis = 1)\n","      da = da.rename(columns={'CIL':'Location','CSID':'Date','PC':'Publication_Count'})\n","      conf = [f'{self.conference}'] * len(da)\n","      da['Conference_Name'] = conf\n","\n","    da.to_csv(\"data_file.csv\")\n","\n","    return da \n","\n","  def grid(self):\n","    da = pd.read_csv(\"data_file.csv\")\n","    address = pd.read_csv('/content/drive/My Drive/AI Index Visualization Project/DATASETS/Conference data/Country data/Grid/addresses.csv')\n","    grid = pd.read_csv('/content/drive/My Drive/AI Index Visualization Project/DATASETS/Conference data/Country data/Grid/grid.csv')\n","    address = address.rename(columns={'grid_id':'ID'})\n","    c = pd.merge(address,grid,how = 'inner', on = ['ID'])\n","    c = c[['Name','City','State','Country','lat','lng']]\n","    c = c.rename(columns={'Name':'Affiliation'})\n","    cc = pd.merge(da,c,how = 'left', on = ['Affiliation'])\n","    cc = cc.fillna('')\n","    cc = cc[cc['lat']!='']\n","    cc.to_csv('data_file.csv')\n","    return cc\n","\n","  \n","\n","def main():\n","  Conference = \"AAAI\" #@param {type : \"string\"}\n","  Start_date = \"1980-01-01\" #@param {type : \"string\"}\n","  End_date = \"2020-01-01\" #@param {type : \"string\"}\n","  Data_to_fetch = \"Publication Count\" #@param [\"Publication Count\",\"Fields of Study\", \"Author\"]\n","  a = MAG(conference = Conference,start_date = Start_date,end_date = End_date,data_fetch=Data_to_fetch,retry=5)\n","  q = a.getData()\n","  q.to_csv(f\"{Conference}.csv\")\n","\n","if __name__ == \"__main__\":\n","    main()\n","    "],"execution_count":null,"outputs":[]}]}